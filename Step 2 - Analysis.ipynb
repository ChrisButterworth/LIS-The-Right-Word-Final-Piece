{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f99ded0-3dde-41b0-ab4b-7a1fa7475993",
   "metadata": {},
   "source": [
    "# Step 2 - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "faba28bd-eebf-4fab-8605-daef4e51b4c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (4.67.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: ollama in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (0.6.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from ollama) (2.12.5)\n",
      "Requirement already satisfied: anyio in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from httpx>=0.27->ollama) (4.11.0)\n",
      "Requirement already satisfied: certifi in /opt/homebrew/opt/certifi/lib/python3.14/site-packages (from httpx>=0.27->ollama) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from httpx>=0.27->ollama) (3.11)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic>=2.9->ollama) (2.41.5)\n",
      "Requirement already satisfied: typing-extensions>=4.14.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic>=2.9->ollama) (4.15.0)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from pydantic>=2.9->ollama) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/homebrew/Cellar/jupyterlab/4.5.0_1/libexec/lib/python3.14/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install tqdm\n",
    "%pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "134487bf-d911-42a7-b03a-308e1eb1c60a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/chrisbutterworth/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Sentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Basic NLP\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41180e1b-1858-45a3-9f01-cfac6ce571ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open CSV file and convert to dataframe\n",
    "df = pd.read_csv('reddit_corpus.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d0514733-85a2-4e7f-a949-b92cd1a09fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 1200/1200 [10:29<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# AI analysis\n",
    "\n",
    "prompt_minzero = \"\"\"You are a helpful research assistant who is analyzing Reddit posts.\n",
    "\n",
    "DEFINITION:\n",
    "Rage bait is content deliberately designed to elicit anger or outrage by being frustrating, provocative, or offensive, typically posted in order to increase traffic to or engagement.\n",
    "\n",
    "TASK:\n",
    "Read the text below and decide whether it contains any rage bait.\n",
    "\n",
    "Give only a one digit response, 1 if the text contains rage bait or 0 if it does not - no additional information is needed.\n",
    "\"\"\"\n",
    "def classify(text, prompt, aimodel):\n",
    "    # Connect to Ollama\n",
    "    response: ChatResponse = chat(model=aimodel, messages=[\n",
    "        {\n",
    "            'role': 'system',\n",
    "            'content': prompt\n",
    "        },\n",
    "        {\n",
    "            'role': 'user',\n",
    "            'content': text,\n",
    "        },\n",
    "    ])\n",
    "\n",
    "    answer = response.message.content\n",
    "\n",
    "    # If the model doesn't 100% follow the prompt and gives an explanation\n",
    "    # let's cut it down to last 3 letters \n",
    "    if (answer != \"1\" and answer != \"0\"):\n",
    "        answer = answer[:1]\n",
    "\n",
    "    return answer\n",
    "\n",
    "def classify_batch(texts, prompt, model=\"deepseek-r1:8b\"):\n",
    "    predictions = []\n",
    "    for text in tqdm(texts):\n",
    "        prediction = classify(text, prompt, model)\n",
    "        predictions.append(prediction)\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Gemma3\n",
    "predictions_gemma = classify_batch(df['text'].tolist(), prompt_minzero, \"gemma3:4b\")\n",
    "df['is_rage_bait'] = predictions_gemma\n",
    "\n",
    "# Update CSV\n",
    "\n",
    "df.to_csv(\"reddit_reviewed_full.csv\", index=False)\n",
    "\n",
    "\"\"\"\n",
    "This was purely used to create a head start and then manually reviewed to double \n",
    "check scores and fix incorrect responses\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "808e7da4-9c17-44f9-95ea-d8983b345b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Questions in rage bait: 51.8%\n",
      "Questions in control: 45.7%\n",
      "820\n",
      "339\n",
      "339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/n7/rn7wk_q95w56p_mhfjyqkyq40000gn/T/ipykernel_17429/1160605263.py:14: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  print(df['is_rage_bait'].value_counts().get(1, 0))\n"
     ]
    }
   ],
   "source": [
    "# Analysis 1 - Question frequency & distribution\n",
    "\n",
    "df['num_questions'] = df['text'].str.count(\"\\\\?\")\n",
    "\n",
    "# Compare rage bait vs. control\n",
    "rage_bait_q_rate = df[df['is_rage_bait']==\"1\"]['has_question'].mean();\n",
    "control_q_rate = df[df['is_rage_bait']==\"0\"]['has_question'].mean()\n",
    "\n",
    "print(f\"Questions in rage bait: {rage_bait_q_rate:.1%}\")\n",
    "print(f\"Questions in control: {control_q_rate:.1%}\")\n",
    "\n",
    "print(df['is_rage_bait'].value_counts().get('1', 0))\n",
    "print(df['is_rage_bait'].value_counts().get('0', 0))\n",
    "# df.to_csv(\"reddit_reviewed_an1.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "490f38c5-4e2e-47d7-bda5-4fd84cbfcd9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rage question type counts\n",
      "question_type\n",
      "other_question      761\n",
      "why_do               33\n",
      "does_anyone_else     26\n",
      "Name: count, dtype: int64\n",
      "Control question type counts\n",
      "question_type\n",
      "other_question      303\n",
      "why_do               28\n",
      "does_anyone_else      8\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Analysis 2 - Question type classification\n",
    "\n",
    "def classify_question_type(text):\n",
    "    text_lower = text.lower()\n",
    "    if 'why do' in text_lower or 'why does' in text_lower:\n",
    "        return 'why_do'\n",
    "    elif 'does anyone' in text_lower or 'is it' in text_lower:\n",
    "        return 'does_anyone_else'\n",
    "    elif 'when did' in text_lower or 'when will' in text_lower:\n",
    "        return 'when_temporal'\n",
    "    elif 'how is' in text_lower and 'not' in text_lower:\n",
    "        return 'how_is_not'\n",
    "    elif \"what's wrong with\" in text_lower:\n",
    "        return 'whats_wrong'\n",
    "    else:\n",
    "        return 'other_question'\n",
    "\n",
    "df['question_type'] = df['text'].apply(classify_question_type)\n",
    "\n",
    "# Analyze distribution\n",
    "rage_type_counts = df[df['is_rage_bait']==\"1\"]['question_type'].value_counts()\n",
    "control_type_counts = df[df['is_rage_bait']==\"0\"]['question_type'].value_counts()\n",
    "\n",
    "df.to_csv(\"reddit_reviewed_an2.csv\", index=False)\n",
    "print('Rage question type counts')\n",
    "print(rage_type_counts)\n",
    "\n",
    "print('Control question type counts')\n",
    "print(control_type_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ddfbc533-867d-485c-b7e2-137b2b569af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "factive_verbs: Rage bait avg=0.01, Control avg=0.03\n",
      "change_of_state: Rage bait avg=0.02, Control avg=0.02\n",
      "temporal: Rage bait avg=0.04, Control avg=0.02\n",
      "universals: Rage bait avg=0.05, Control avg=0.03\n"
     ]
    }
   ],
   "source": [
    "# Analysis 3: Presupposition trigger counting\n",
    "\n",
    "presupposition_triggers = {\n",
    "    'factive_verbs': ['know', 'realize', 'forget', 'remember', 'regret', 'aware'],\n",
    "    'change_of_state': ['stop', 'start', 'become', 'cease', 'continue', 'turn'],\n",
    "    'temporal': ['still', 'always', 'never', 'anymore', 'yet'],\n",
    "    'universals': ['everyone', 'all', 'every', 'nobody', 'none', 'no one']\n",
    "}\n",
    "\n",
    "def count_triggers(text, trigger_list):\n",
    "    count = 0\n",
    "    text_lower = text.lower()\n",
    "    for trigger in trigger_list:\n",
    "        count += len(re.findall(r'\\b' + trigger + r'\\b', text_lower))\n",
    "    return count\n",
    "\n",
    "for category, triggers in presupposition_triggers.items():\n",
    "    df[f'{category}_count'] = df['text'].apply(lambda x: count_triggers(x, triggers))\n",
    "\n",
    "# Compare rage bait vs. control\n",
    "for category in presupposition_triggers.keys():\n",
    "    rage_bait_avg = df[df['is_rage_bait']==\"1\"][f'{category}_count'].mean()\n",
    "    control_avg = df[df['is_rage_bait']==\"0\"][f'{category}_count'].mean()\n",
    "    print(f\"{category}: Rage bait avg={rage_bait_avg:.2f}, Control avg={control_avg:.2f}\")\n",
    "\n",
    "df.to_csv(\"reddit_reviewed_an3.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5d4b8d31-dd85-4413-a00b-1556b90ae136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rage bait sentiment: -0.036\n",
      "Control sentiment: 0.062\n"
     ]
    }
   ],
   "source": [
    "# Analysis 4: Sentiment analysis\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment(text):\n",
    "    scores = analyzer.polarity_scores(text)\n",
    "    return scores['compound']  # -1 (very negative) to +1 (very positive)\n",
    "\n",
    "df['sentiment'] = df['text'].apply(get_sentiment)\n",
    "\n",
    "# Compare\n",
    "print(f\"Rage bait sentiment: {df[df['is_rage_bait']==\"1\"]['sentiment'].mean():.3f}\")\n",
    "print(f\"Control sentiment: {df[df['is_rage_bait']==\"0\"]['sentiment'].mean():.3f}\")\n",
    "\n",
    "df.to_csv(\"reddit_review_an4.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f24ff0e0-e8c6-4919-9022-a9a4ba03fc6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correlations with engagement (comments):\n",
      "\n",
      "Question count                 r = -0.083  (p = 0.004) **\n",
      "Question presence              r = -0.087  (p = 0.003) **\n",
      "Sentiment                      r = -0.018  (p = 0.525) ns\n",
      "Universal quantifiers          r =  0.069  (p = 0.018) *\n",
      "Temporal markers               r = -0.007  (p = 0.801) ns\n",
      "Factive verbs                  r = -0.004  (p = 0.897) ns\n",
      "Change-of-state verbs          r = -0.014  (p = 0.640) ns\n"
     ]
    }
   ],
   "source": [
    "# Analysis 5 - Engagement correlation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Engagement metrics (use whichever you have)\n",
    "engagement_metric = 'num_comments'  # or 'upvotes' or combined score\n",
    "\n",
    "# Features to test\n",
    "features = {\n",
    "    'num_questions': 'Question count',\n",
    "    'has_question': 'Question presence',\n",
    "    'sentiment': 'Sentiment',\n",
    "    'universals_count': 'Universal quantifiers',\n",
    "    'temporal_count': 'Temporal markers',\n",
    "    'factive_verbs_count': 'Factive verbs',\n",
    "    'change_of_state_count': 'Change-of-state verbs'\n",
    "}\n",
    "\n",
    "print(\"Correlations with engagement (comments):\\n\")\n",
    "results = {}\n",
    "\n",
    "for feature, label in features.items():\n",
    "    if feature in df.columns:\n",
    "        corr = df[feature].corr(df[engagement_metric])\n",
    "        # Statistical significance\n",
    "        _, p_value = stats.pearsonr(df[feature].dropna(), \n",
    "                                     df[engagement_metric].dropna())\n",
    "        results[label] = {'r': corr, 'p': p_value}\n",
    "        sig = \"***\" if p_value < .001 else \"**\" if p_value < .01 else \"*\" if p_value < .05 else \"ns\"\n",
    "        print(f\"{label:30s} r = {corr:6.3f}  (p = {p_value:.3f}) {sig}\")\n",
    "\n",
    "# Sort by correlation strength\n",
    "sorted_results = sorted(results.items(), key=lambda x: abs(x[1]['r']), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9a26c-09b1-478a-9edd-f531aaf15063",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
